{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram review sentiment analysis\n",
    "___\n",
    "\n",
    "> ### If you are using jupyter notebook install below package by running cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and wait until it installed successfully ! \n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### If you are using python in cmd (command prompt) install below package in terminal using\n",
    "\n",
    "<br>\n",
    "\n",
    "```pip install vanderSentiment```\n",
    "\n",
    "<br>\n",
    "                                 \n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Single review prediction :\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T10:07:09.156143Z",
     "start_time": "2019-07-25T10:07:09.120141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.218, 'neu': 0.663, 'pos': 0.118, 'compound': -0.7351}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score\n",
    "\n",
    "\n",
    "sentiment_analyzer_scores('PUBLICATION: A few reasons why the Pacific Islands are at high risk of debt distress: üìàModest long-term economic growth prospects üåÄHigh vulnerability to natural disasters üöß High costs for public services/infrastructure. ‚óæÔ∏è‚óæÔ∏è‚óæÔ∏è‚óæÔ∏è‚óæÔ∏èRead more by copying this link or type \"East Asia & Pacific Economic Update 2019\" on your search browser. #EAPUpdate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi review prediction at once :\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T10:13:56.846461Z",
     "start_time": "2019-07-25T10:13:53.757285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T10:14:25.429096Z",
     "start_time": "2019-07-25T10:13:56.850462Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sifi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#loading dataset\n",
    "df = pd.read_excel('insta-data-clean.xlsx')\n",
    "\n",
    "\n",
    "# Generating empty dataset having following columns beacuse we have to values in it according to our sentiment scores\n",
    "df_final = pd.DataFrame(columns=['Caption post','Negative','Neutral','Possitive','Overall Sentiment'])\n",
    "\n",
    "df.drop(index=df[df[\"Post content\"].isna()].index,axis=0,inplace=True)\n",
    "\n",
    "\n",
    "collect = defaultdict(list)\n",
    "check_record_for_none = index = df[df[\"Post content\"].isna()]\n",
    "\n",
    "if check_record_for_none.index > 0:\n",
    "  print('NONE value present at location {}'.format(check_index.index))\n",
    "  df.drop(index=df[df[\"Post content\"].isna()].index,axis=0,inplace=True)\n",
    "\n",
    "\n",
    "for record in df.iterrows():\n",
    "  record = record[1][\"Post content\"]\n",
    "  collect[\"Caption post\"].append(df[df['Post content'] == record][\"Post content\"].get_values()[0])\n",
    "  review = sentiment_analyzer_scores(record)\n",
    "  #print(type(review))\n",
    "  #print(sub.findall(review))\n",
    "# print(review)\n",
    "  collect[\"Negative\"].append(review[\"neg\"])\n",
    "  collect[\"Neutral\"].append(review[\"neu\"])\n",
    "  collect[\"Possitive\"].append(review[\"pos\"])\n",
    "  collect[\"Overall Sentiment\"].append(review[\"compound\"])\n",
    "  #break\n",
    "new_col = pd.DataFrame(collect)\n",
    "new_col = pd.DataFrame(new_col)\n",
    "\n",
    "df[df['Post content'] == record][\"Post content\"].get_values()[0]\n",
    "df[\"Post content\"]\n",
    "check_index = index=df[df[\"Post content\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T10:15:02.810234Z",
     "start_time": "2019-07-25T10:15:02.769232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caption post</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Possitive</th>\n",
       "      <th>Overall Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUBLICATION: A few reasons why the Pacific Isl...</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.7351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With 6 million #solar panels, a new solar plan...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Millions of people in #Mozambique, #Malawi and...</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.8519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What kind of #Data interests you? Tell us in t...</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.8119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CALLING ALL ARTISTS! This one's for you! üé®üéµüéûÔ∏èüñº...</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.9182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Caption post  Negative  Neutral  \\\n",
       "0  PUBLICATION: A few reasons why the Pacific Isl...     0.218    0.663   \n",
       "1  With 6 million #solar panels, a new solar plan...     0.027    0.911   \n",
       "2  Millions of people in #Mozambique, #Malawi and...     0.127    0.823   \n",
       "3  What kind of #Data interests you? Tell us in t...     0.091    0.891   \n",
       "4  CALLING ALL ARTISTS! This one's for you! üé®üéµüéûÔ∏èüñº...     0.042    0.817   \n",
       "\n",
       "   Possitive  Overall Sentiment  \n",
       "0      0.118            -0.7351  \n",
       "1      0.062             0.4404  \n",
       "2      0.050            -0.8519  \n",
       "3      0.017            -0.8119  \n",
       "4      0.142             0.9182  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 records\n",
    "new_col.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T10:17:15.231808Z",
     "start_time": "2019-07-25T10:17:15.208807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caption post</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Possitive</th>\n",
       "      <th>Overall Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>We took a look at the US economy. Our findings...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>#tbt British economist John Maynard Keynes and...</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.7430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Global economy on firm footing; growth project...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Indonesia‚Äôs Minister of Finance Sri Mulyani an...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Just completed Germany‚Äôs annual economic revie...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Caption post  Negative  Neutral  \\\n",
       "889  We took a look at the US economy. Our findings...     0.000    1.000   \n",
       "890  #tbt British economist John Maynard Keynes and...     0.058    0.813   \n",
       "891  Global economy on firm footing; growth project...     0.000    0.867   \n",
       "892  Indonesia‚Äôs Minister of Finance Sri Mulyani an...     0.000    1.000   \n",
       "893  Just completed Germany‚Äôs annual economic revie...     0.000    1.000   \n",
       "\n",
       "     Possitive  Overall Sentiment  \n",
       "889      0.000             0.0000  \n",
       "890      0.129             0.7430  \n",
       "891      0.133             0.3818  \n",
       "892      0.000             0.0000  \n",
       "893      0.000             0.0000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last 5 records\n",
    "new_col.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Saving result in Csv File \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T10:17:37.105059Z",
     "start_time": "2019-07-25T10:17:37.099059Z"
    }
   },
   "outputs": [],
   "source": [
    "new_col.to_csv('Saving_all_prediction_in_CSV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Saving result in Excel File\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col.to_csv('Saving_all_prediction_in_EXCEL.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SO HOW IT WORKS ?\n",
    "___\n",
    "\n",
    "<img src=\"https://www.lexalytics.com/images/extra/sentiment1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:poppins\">\n",
    "    <b>VADER (Valence Aware Dictionary and sEntiment Reasoner)</b> is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:poppins\">\n",
    "examples of typical use cases for sentiment analysis, including proper handling of sentences with:\n",
    "\n",
    "> typical negations (e.g., \"not good\")\n",
    "\n",
    "> use of contractions as negations (e.g., \"wasn't very good\")\n",
    "\n",
    "> conventional use of punctuation to signal increased sentiment intensity (e.g., \"Good!!!\")\n",
    "\n",
    "> conventional use of word-shape to signal emphasis (e.g., using ALL CAPS for words/phrases)\n",
    "\n",
    "> using degree modifiers to alter sentiment intensity (e.g., intensity boosters such as \"very\" and \n",
    "\n",
    "> intensity dampeners such as \"kind of\")\n",
    "\n",
    "> understanding many sentiment-laden slang words (e.g., 'sux')\n",
    "\n",
    "> understanding many sentiment-laden slang words as modifiers such as 'uber' or 'friggin' or 'kinda'\n",
    "\n",
    "> understanding many sentiment-laden emoticons such as :) and :D\n",
    "\n",
    "> translating utf-8 encoded emojis such as cupid and kiss and grin\n",
    "\n",
    "> understanding sentiment-laden initialisms and acronyms (for example: 'lol')\n",
    "\n",
    "> more examples of tricky sentences that confuse other sentiment analysis tools\n",
    "\n",
    "> example for how VADER can work in conjunction with NLTK to do sentiment analysis on longer texts...i.e., decomposing paragraphs, articles/reports/publications, or novels into sentence-level analyses\n",
    "\n",
    ">  of a concept for assessing the sentiment of images, video, or other tagged multimedia content\n",
    "\n",
    "> if you have access to the Internet, the demo has an example of how VADER can work with analyzing sentiment of texts in other languages (non-English text sentences).\n",
    "\n",
    "</p>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/bb5704679d1aaafd4fabfbe8b34930a98d40714f/2-Figure1-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.mathworks.com/products/text-analytics/_jcr_content/mainParsys/band_copy_688706585__253862225/mainParsys/columns/2/image_copy_copy.adapt.full.high.gif/1559064584467.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subgroup: face-positive\n",
    "\n",
    "1F600                                      ; fully-qualified     # üòÄ grinning face\n",
    "\n",
    "1F601                                      ; fully-qualified     # üòÅ beaming face with smiling eyes\n",
    "\n",
    "1F602                                      ; fully-qualified     # üòÇ face with tears of joy\n",
    "\n",
    "1F923                                      ; fully-qualified     # ü§£ rolling on the floor laughing\n",
    "\n",
    "1F603                                      ; fully-qualified     # üòÉ grinning face with big eyes\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "(more emoji code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Their is meaning for all expression in numerical format\n",
    "\n",
    "( '}{' )\t1.6\t0.66332\t[1, 2, 2, 1, 1, 2, 2, 1, 3, 1]\n",
    "\n",
    "(%\t-0.9\t0.9434\t[0, 0, 1, -1, -1, -1, -2, -2, -1, -2]\n",
    " \n",
    "('-:\t2.2\t1.16619\t[4, 1, 4, 3, 1, 2, 3, 1, 2, 1]\n",
    "\n",
    "(':\t2.3\t0.9\t[1, 3, 3, 2, 2, 4, 2, 3, 1, 2]\n",
    "\n",
    "((-:\t2.1\t0.53852\t[2, 2, 2, 1, 2, 3, 2, 2, 3, 2]\n",
    "\n",
    "(*\t1.1\t1.13578\t[2, 1, 1, -1, 1, 2, 2, -1, 2, 2]\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "(more emoji code)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='http://www.joshuakim.io/wp-content/uploads/2017/12/filtering2.jpg'/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Suthendran_Kannan/publication/325896826/figure/fig5/AS:639911610818561@1529578225737/Sentiment-Analysis-vs-VADER-Sentiment-Analysis.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Scoring \n",
    "____\n",
    "\n",
    "The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). \n",
    "\n",
    "This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    "> ### positive sentiment: compound score >= 0.05\n",
    "\n",
    "> ### neutral sentiment: compound score > -0.05 and compound score < 0.05\n",
    "\n",
    "> ### negative sentiment: compound score <= -0.05\n",
    "\n",
    "<hr>\n",
    "\n",
    "The ```pos, neu, and neg``` scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation). \n",
    "\n",
    "These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
